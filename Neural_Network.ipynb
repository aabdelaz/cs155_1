{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiexinchen/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Neural network implementation\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from process_data_helper import (\n",
    "    load_data,\n",
    "    process_training_data,\n",
    "    process_testing_data,\n",
    "    write_predictions,\n",
    "    process_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = load_data(\"training_data.txt\")\n",
    "test_data = load_data(\"test_data.txt\")\n",
    "X_train, X_test, Y_train = process_data(train_data, test_data, 'tfidf', add_bias=False)\n",
    "X_train_l1norm, Y_train_l1norm = process_training_data(train_data, 'l1', add_bias = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10107.0\n",
      "10107.0\n",
      "1.0\n",
      "1.0\n",
      "0.7246918984798962\n"
     ]
    }
   ],
   "source": [
    "#print(Y_train)\n",
    "print(Y_train.sum())\n",
    "print(Y_train_l1norm.sum())\n",
    "print(np.amax(X_train))\n",
    "print(np.amax(X_train_l1norm))\n",
    "print(np.amax((X_train-X_train_l1norm)))\n",
    "#X_train = X_train_l1norm\n",
    "'''\n",
    "thoughts for next-step:\n",
    "1. try l1 norm(applied to training and test together) for NN\n",
    "2. try different activation function\n",
    "3. try different layer numbers\n",
    "\n",
    "to-check:\n",
    "1. processed data:\n",
    "    are the X too small to be learned? -> try scale them up?\n",
    "2. output prediction: are they always 0/1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_208 (Dense)            (None, 70)                70070     \n",
      "_________________________________________________________________\n",
      "activation_208 (Activation)  (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "dense_209 (Dense)            (None, 1)                 71        \n",
      "_________________________________________________________________\n",
      "activation_209 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 70,141\n",
      "Trainable params: 70,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "L1 = 70\n",
    "L2 = 25\n",
    "L3 = 10\n",
    "dropout_prob1 = 0.13\n",
    "dropout_prob2 = 0.08\n",
    "dropout_prob3 = 0.0\n",
    "ACTIVATION = 'relu'\n",
    "model = []\n",
    "model = Sequential()\n",
    "model.add(Dense(L1, input_shape=(X_train.shape[1],)))\n",
    "model.add(Activation(ACTIVATION))\n",
    "model.add(Dropout(dropout_prob1))\n",
    "\n",
    "'''\n",
    "model.add(Dense(L2))\n",
    "model.add(Activation(ACTIVATION))\n",
    "model.add(Dropout(dropout_prob2))\n",
    "'''\n",
    "'''\n",
    "model.add(Dense(L3))\n",
    "model.add(Activation(ACTIVATION))\n",
    "model.add(Dropout(dropout_prob3))\n",
    "'''\n",
    "#the output layer\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "## Printing a summary of the layers and weights in your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.4424 - acc: 0.7865\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 0.3230 - acc: 0.8605\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 0.2881 - acc: 0.8792\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 0.2464 - acc: 0.9039\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "fit = model.fit(X_train, Y_train, batch_size=64, epochs=4,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_data = load_data(\"test_data.txt\")\n",
    "#X_test = process_testing_data(test_data, 'tfidf', add_bias=False)\n",
    "write_predictions(model, X_train, \"train_predictions.txt\")\n",
    "write_predictions(model, X_test, \"test_predictions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "16000/16000 [==============================] - 6s 348us/step - loss: 0.5155 - acc: 0.8124\n",
      "Epoch 2/5\n",
      "16000/16000 [==============================] - 1s 78us/step - loss: 0.3453 - acc: 0.8561\n",
      "Epoch 3/5\n",
      "16000/16000 [==============================] - 1s 76us/step - loss: 0.3180 - acc: 0.8672\n",
      "Epoch 4/5\n",
      "16000/16000 [==============================] - 1s 83us/step - loss: 0.3081 - acc: 0.8698\n",
      "Epoch 5/5\n",
      "16000/16000 [==============================] - 1s 86us/step - loss: 0.3051 - acc: 0.8714\n",
      "Epoch 1/5\n",
      "16000/16000 [==============================] - 6s 358us/step - loss: 0.5178 - acc: 0.8004\n",
      "Epoch 2/5\n",
      "16000/16000 [==============================] - 1s 79us/step - loss: 0.3415 - acc: 0.8594\n",
      "Epoch 3/5\n",
      "16000/16000 [==============================] - 1s 89us/step - loss: 0.3147 - acc: 0.8668\n",
      "Epoch 4/5\n",
      "16000/16000 [==============================] - 1s 86us/step - loss: 0.3071 - acc: 0.8679\n",
      "Epoch 5/5\n",
      "16000/16000 [==============================] - 1s 89us/step - loss: 0.3028 - acc: 0.8716\n",
      "Epoch 1/5\n",
      "16000/16000 [==============================] - 6s 353us/step - loss: 0.5246 - acc: 0.8057\n",
      "Epoch 2/5\n",
      "16000/16000 [==============================] - 1s 89us/step - loss: 0.3434 - acc: 0.8600\n",
      "Epoch 3/5\n",
      "16000/16000 [==============================] - 1s 91us/step - loss: 0.3134 - acc: 0.8663\n",
      "Epoch 4/5\n",
      "16000/16000 [==============================] - 1s 84us/step - loss: 0.3041 - acc: 0.8717\n",
      "Epoch 5/5\n",
      "16000/16000 [==============================] - 1s 83us/step - loss: 0.3002 - acc: 0.8733\n",
      "Epoch 1/5\n",
      "16000/16000 [==============================] - 6s 374us/step - loss: 0.5130 - acc: 0.8082\n",
      "Epoch 2/5\n",
      "16000/16000 [==============================] - 1s 84us/step - loss: 0.3381 - acc: 0.8594\n",
      "Epoch 3/5\n",
      "16000/16000 [==============================] - 1s 79us/step - loss: 0.3107 - acc: 0.8664\n",
      "Epoch 4/5\n",
      "16000/16000 [==============================] - 1s 83us/step - loss: 0.3028 - acc: 0.8726\n",
      "Epoch 5/5\n",
      "16000/16000 [==============================] - 1s 84us/step - loss: 0.2982 - acc: 0.8720\n",
      "Epoch 1/5\n",
      "16000/16000 [==============================] - 6s 373us/step - loss: 0.5185 - acc: 0.8057\n",
      "Epoch 2/5\n",
      "16000/16000 [==============================] - 1s 84us/step - loss: 0.3431 - acc: 0.8596\n",
      "Epoch 3/5\n",
      "16000/16000 [==============================] - 1s 84us/step - loss: 0.3150 - acc: 0.8654\n",
      "Epoch 4/5\n",
      "16000/16000 [==============================] - 1s 90us/step - loss: 0.3075 - acc: 0.8693\n",
      "Epoch 5/5\n",
      "16000/16000 [==============================] - 1s 87us/step - loss: 0.3031 - acc: 0.8723\n"
     ]
    }
   ],
   "source": [
    "#KFold method to estimate the test error\n",
    "KFOLD = 5\n",
    "kf = KFold(n_splits = KFOLD)\n",
    "inds = [ind for ind in kf.split(X_train, Y_train)]\n",
    "train_err = np.zeros((KFOLD, ))\n",
    "test_err = np.zeros((KFOLD, ))\n",
    "train_accuracy = np.zeros((KFOLD, ))\n",
    "test_accuracy = np.zeros((KFOLD, ))\n",
    "err_train_total = 0\n",
    "err_val_total = 0\n",
    "model_Kfold = []\n",
    "fit = []\n",
    "model_kfold = []\n",
    "'''\n",
    "L1 = 450\n",
    "L2 = 150\n",
    "L3 = 15\n",
    "dropout_prob1 = 0.14\n",
    "dropout_prob2 = 0.1\n",
    "dropout_prob3 = 0.0\n",
    "'''\n",
    "for kfold_iter in range(KFOLD):\n",
    "    model_kfold.append(Sequential())\n",
    "    model_kfold[kfold_iter].add(Dense(L1, input_shape=(X_train.shape[1],)))\n",
    "    model_kfold[kfold_iter].add(Activation('relu'))\n",
    "    model_kfold[kfold_iter].add(Dropout(dropout_prob1))\n",
    "    '''\n",
    "    model_kfold[kfold_iter].add(Dense(L2))\n",
    "    model_kfold[kfold_iter].add(Activation('relu'))\n",
    "    model_kfold[kfold_iter].add(Dropout(dropout_prob2))\n",
    "    '''\n",
    "    '''\n",
    "    model_kfold[kfold_iter].add(Dense(L3))\n",
    "    model_kfold[kfold_iter].add(Activation('relu'))\n",
    "    dropout_prob = dropout_prob3\n",
    "    model_kfold[kfold_iter].add(Dropout(dropout_prob3))\n",
    "    '''\n",
    "    #the output layer\n",
    "    model_kfold[kfold_iter].add(Dense(1))\n",
    "    model_kfold[kfold_iter].add(Activation('sigmoid'))\n",
    "    train, val = inds[kfold_iter]\n",
    "    X_train_Kfold = X_train[train, :]\n",
    "    Y_train_Kfold = Y_train[train, :]\n",
    "    X_test_Kfold = X_train[val, :]\n",
    "    Y_test_Kfold = Y_train[val, :]\n",
    "    model_kfold[kfold_iter].compile(loss='binary_crossentropy',\n",
    "                         optimizer='RMSprop', metrics=['accuracy'])\n",
    "    fit.append(model_kfold[kfold_iter].fit(X_train_Kfold, Y_train_Kfold, batch_size=64, epochs=5,\n",
    "                    verbose=1))\n",
    "    train_score = model_kfold[kfold_iter].evaluate(X_train_Kfold, Y_train_Kfold, verbose=0)\n",
    "    test_score = model_kfold[kfold_iter].evaluate(X_test_Kfold, Y_test_Kfold, verbose=0)\n",
    "    train_err[kfold_iter] = train_score[0]\n",
    "    test_err[kfold_iter] = test_score[0]\n",
    "    train_accuracy[kfold_iter] = train_score[1]\n",
    "    test_accuracy[kfold_iter] = test_score[1]\n",
    "#err_train_total += train_err\n",
    "#err_val_total += val_err\n",
    "#err_train[dim_iter, N_iter] = err_train_total/5\n",
    "#err_val[dim_iter, N_iter] = err_val_total/5\n",
    "\n",
    "#score = modelE.evaluate(X_test_vec, y_test_onehot, verbose=0)\n",
    "#print('Test score:', score[0])\n",
    "#print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.880175\n",
      "0.8501000000000001\n",
      "[0.28942222 0.28727808 0.28513491 0.28351232 0.2889    ]\n",
      "[0.32588966 0.34272728 0.35206293 0.35486643 0.33979274]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_accuracy))\n",
    "print(np.mean(test_accuracy))\n",
    "print(train_err)\n",
    "print(test_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000,)\n",
      "(4000,)\n",
      "(16000, 1000)\n",
      "(16000, 1)\n",
      "(4000, 1000)\n",
      "(4000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(X_train_Kfold.shape)\n",
    "print(Y_train_Kfold.shape)\n",
    "print(X_test_Kfold.shape)\n",
    "print(Y_test_Kfold.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 4000,  4001,  4002, ..., 19997, 19998, 19999]),\n",
       "  array([   0,    1,    2, ..., 3997, 3998, 3999])),\n",
       " (array([    0,     1,     2, ..., 19997, 19998, 19999]),\n",
       "  array([4000, 4001, 4002, ..., 7997, 7998, 7999])),\n",
       " (array([    0,     1,     2, ..., 19997, 19998, 19999]),\n",
       "  array([ 8000,  8001,  8002, ..., 11997, 11998, 11999])),\n",
       " (array([    0,     1,     2, ..., 19997, 19998, 19999]),\n",
       "  array([12000, 12001, 12002, ..., 15997, 15998, 15999])),\n",
       " (array([    0,     1,     2, ..., 15997, 15998, 15999]),\n",
       "  array([16000, 16001, 16002, ..., 19997, 19998, 19999]))]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
